{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOlNZgOafs5b"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Object detection model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGYgvC7P7faD"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzM-FLsJKwij"
      },
      "source": [
        "The MediaPipe object detection solution provides several models you can use immediately for machine learning (ML) in your application. However, if you need to detect objects not covered by the provided models, you can customize any of the provided models with your own data and MediaPipe Model Maker. This model modification tool rebuilds the model using data you provide. This method is faster than training a new model and can produce a model that is more useful for your specific application.\n",
        "\n",
        "The following sections show you how to use Model Maker to retrain a pre-built model for object detection with your own data, which you can then use with the MediaPipe [Object Detector](https://developers.google.com/mediapipe/solutions/vision/object_detector). The example retrains a general purpose object detection model to detect android figurines in images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_9q7fppLbPA"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section describes key steps for setting up your development environment to retrain a model. These instructions describe how to update a model using [Google Colab](https://colab.research.google.com/), and you can also use Python in your own development environment. For general information on setting up your development environment for using MediaPipe, including platform version requirements, see the [Setup guide for Python](https://developers.google.com/mediapipe/solutions/setup_python).\n",
        "\n",
        "**Attention:** This MediaPipe Solutions Preview is an early release. [Learn more](https://developers.google.com/mediapipe/solutions/about).\n",
        "\n",
        "To install the libraries for customizing a model, run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMSCcHh1LScM"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker\n",
        "!pip install roboflow  # 需要用其下载数据集\n",
        "!pip install comet_ml  # 需要用其记录训练数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXvZgLPLh6n"
      },
      "source": [
        "Use the following code to import the required Python classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oazmbPzKHYFq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "from mediapipe_model_maker import object_detector\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "实验数据记录"
      ],
      "metadata": {
        "id": "L_WKOix8oAOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from comet_ml import start\n",
        "from comet_ml.integration.pytorch import log_model\n",
        "\n",
        "experiment = start(\n",
        "  api_key=\"L9Hl7JDyQloSScOBXqjixAq09\",\n",
        "  project_name=\"volley-detect\",\n",
        "  workspace=\"liyunkai13\"\n",
        ")\n",
        "hyper_params = {\n",
        "   \"learning_rate\": 0.5,\n",
        "   \"steps\": 100000,\n",
        "   \"batch_size\": 50,\n",
        "}\n",
        "experiment.log_parameters(hyper_params)"
      ],
      "metadata": {
        "id": "Kll6clxUlYdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zP1AkaRL72Z"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVnSV0ZQMA5-"
      },
      "source": [
        "Retraining a model for object detection requires a dataset that includes the items, or classes, that you want the completed model to be able to identify. You can do this by trimming down a public dataset to only the classes that are relevant to your usecase, compiling your own dataset, or some combination of both, The dataset can be significantly smaller than what would be required to train a new model. For example, the [COCO](https://cocodataset.org/) dataset used to train many reference models contains hundreds of thousands of images with 91 classes of objects. Transfer learning with Model Maker can retrain an existing model with a smaller dataset and still perform well, depending on your inference accuracy goals. These instructions use a smaller dataset containing 2 types of android figurines, or 2 classes, with 62 total training images.\n",
        "\n",
        "To download the example dataset, use the following code:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 从Roboflow下载数据集"
      ],
      "metadata": {
        "id": "mBiEwsI6S_Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"83pzRFncTtpun9yDYi5f\")\n",
        "project = rf.workspace(\"volleyball-mrfzh\").project(\"volley_detect\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")\n",
        ""
      ],
      "metadata": {
        "id": "2k0N-y4PSvAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 从谷歌云盘获取数据集"
      ],
      "metadata": {
        "id": "GABTP37HTJa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz3-eHe07FEX"
      },
      "outputs": [],
      "source": [
        "# 把谷歌云盘挂载在虚拟机上\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 把数据局文件复制到本地目录\n",
        "!cp \"/content/drive/MyDrive/Volleyball.v1i.coco.zip\" \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ELgPgBa3TZQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 若为压缩包，还需要解压"
      ],
      "metadata": {
        "id": "n-q7mmpUTcmS"
      }
    },
    {
      "source": [
        "# 解压数据集\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/Volleyball.v1i.coco.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/dataset')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dphDBD_U1KbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEVcacUj7L1l"
      },
      "source": [
        "This code stores the dataset at the directory location `android_figurine`. The directory contains two subdirectories for the training and validation datasets, located in `android_figurine/train` and `android_figurine/validation` respectively. Each of the train and validation datasets follow the COCO Dataset format described below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbIAPjPUMHiK"
      },
      "source": [
        "### 支持的数据集格式\n",
        "Model Maker Object Detection API 支持读取这样的数据集格式:\n",
        "\n",
        "#### COCO format\n",
        "\n",
        "COCO 格式的数据集有一个 data 文件夹，其包含所有的图片和一个  `labels.json` 文件，这个文件包含着所有图片的对象标注\n",
        "```\n",
        "<dataset_dir>/\n",
        "  data/\n",
        "    <img0>.<jpg/jpeg>\n",
        "    <img1>.<jpg/jpeg>\n",
        "    ...\n",
        "  labels.json\n",
        "```\n",
        "where `labels.json` is formatted as:\n",
        "```\n",
        "{\n",
        "  \"categories\":[\n",
        "    {\"id\":1, \"name\":<volleyball>},\n",
        "    ...\n",
        "  ],\n",
        "  \"images\":[\n",
        "    {\"id\":0, \"file_name\":\"<img0>.<jpg/jpeg>\"},\n",
        "    ...\n",
        "  ],\n",
        "  \"annotations\":[\n",
        "    {\"id\":0, \"image_id\":0, \"category_id\":1, \"bbox\":[x-top left, y-top left, width, height]},\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### PASCAL VOC format\n",
        "\n",
        "The PASCAL VOC dataset format also has a `data` directory which stores all of the images, however the annotations are split up per image into corresponding xml files in the `Annotations` directory.\n",
        "```\n",
        "<dataset_dir>/\n",
        "  data/\n",
        "    <file0>.<jpg/jpeg>\n",
        "    ...\n",
        "  Annotations/\n",
        "    <file0>.xml\n",
        "    ...\n",
        "```\n",
        "where the xml files are formatted as:\n",
        "```\n",
        "<annotation>\n",
        "  <filename>file0.jpg</filename>\n",
        "  <object>\n",
        "    <name>kangaroo</name>\n",
        "    <bndbox>\n",
        "      <xmin>233</xmin>\n",
        "      <ymin>89</ymin>\n",
        "      <xmax>386</xmax>\n",
        "      <ymax>262</ymax>\n",
        "    </bndbox>\n",
        "  </object>\n",
        "  <object>\n",
        "    ...\n",
        "  </object>\n",
        "  ...\n",
        "</annotation>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TPn8Mb_aJb"
      },
      "source": [
        "### 数据集预览"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L64U7mgPNKec"
      },
      "source": [
        "从json文件中打印类别来验证数据集的内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f_Z-TAwNK3n"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(\"/content/volley_detect-1/train\",\"_annotations.coco.json\"), \"r\") as f:\n",
        "  labels_json = json.load(f)\n",
        "for category_item in labels_json[\"categories\"]:\n",
        "  print(f\"{category_item['id']}: {category_item['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr-7QJ05PmyS"
      },
      "source": [
        "为了更好地理解数据集，绘制两个示例图像及其边框"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kTw3uodPl7-",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "# @title Visualize the training dataset {\"vertical-output\":true}\n",
        "variable_name = \"\" # @param {\"type\":\"string\"}\n",
        "variable_name = \"\" # @param {\"type\":\"raw\"}\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, text, patheffects\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "def draw_outline(obj):\n",
        "  obj.set_path_effects([patheffects.Stroke(linewidth=4,  foreground='black'), patheffects.Normal()])\n",
        "def draw_box(ax, bb):\n",
        "  patch = ax.add_patch(patches.Rectangle((bb[0],bb[1]), bb[2], bb[3], fill=False, edgecolor='red', lw=2))\n",
        "  draw_outline(patch)\n",
        "def draw_text(ax, bb, txt, disp):\n",
        "  text = ax.text(bb[0],(bb[1]-disp),txt,verticalalignment='top'\n",
        "  ,color='white',fontsize=10,weight='bold')\n",
        "  draw_outline(text)\n",
        "def draw_bbox(ax, annotations_list, id_to_label, image_shape):\n",
        "  for annotation in annotations_list:\n",
        "    cat_id = annotation[\"category_id\"]\n",
        "    bbox = annotation[\"bbox\"]\n",
        "    draw_box(ax, bbox)\n",
        "    draw_text(ax, bbox, id_to_label[cat_id], image_shape[0] * 0.05)\n",
        "def visualize(dataset_folder, max_examples=None):\n",
        "  with open(os.path.join(dataset_folder,\"train\",\"_annotations.coco.json\"), \"r\") as f:\n",
        "    labels_json = json.load(f)\n",
        "  images = labels_json[\"images\"]\n",
        "  cat_id_to_label = {item[\"id\"]:item[\"name\"] for item in labels_json[\"categories\"]}\n",
        "  image_annots = defaultdict(list)\n",
        "  for annotation_obj in labels_json[\"annotations\"]:\n",
        "    image_id = annotation_obj[\"image_id\"]\n",
        "    image_annots[image_id].append(annotation_obj)\n",
        "\n",
        "  if max_examples is None:\n",
        "    max_examples = len(image_annots.items())\n",
        "  n_rows = math.ceil(max_examples / 3)\n",
        "  fig, axs = plt.subplots(n_rows, 3, figsize=(24, n_rows*8)) # 3 columns(2nd index), 8x8 for each image\n",
        "  for ind, (image_id, annotations_list) in enumerate(list(image_annots.items())[:max_examples]):\n",
        "    ax = axs[ind//3, ind%3]\n",
        "    img = plt.imread(os.path.join(dataset_folder, \"train\", images[image_id][\"file_name\"]))\n",
        "    ax.imshow(img)\n",
        "    draw_bbox(ax, annotations_list, cat_id_to_label, img.shape)\n",
        "  plt.show()\n",
        "\n",
        "# visualize(train_dataset_path, 9)\n",
        "visualize(\"/content/volley_detect-1\", 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANqfl-ghQM74"
      },
      "source": [
        "### 创建数据集\n",
        "\n",
        "数据集类有从COCO和PASCAL VOC数据集中加载的两种方法:\n",
        "* `Dataset.from_coco_folder`\n",
        "* `Dataset.from_pascal_voc_folder`\n",
        "\n",
        "由于所用数据集是COCO格式，使用 `from_coco_folder`方法来加载位于 `train_dataset_path` and `validation_dataset_path`两个路径下的数据集。加载数据集时，数据会被转换成标准的 [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)格式，被缓存作之后使用。应该创建一个 `cache_dir`文件夹并复用其在每一次训练中，避免存储同一个数据集的多个缓存。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 数据集组织格式处理"
      ],
      "metadata": {
        "id": "CNf4D_3cU_5x"
      }
    },
    {
      "source": [
        "# 因为数据集内格式与from_coco_folder方法的预想格式一致，写个脚本把train下边的所有图片\n",
        "# 放在train/images中\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_images_to_subfolder(data_dir, subfolder_name=\"images\"):\n",
        "  \"\"\"将指定目录下的所有图像文件移动到名为 subfolder_name 的子目录中。\n",
        "\n",
        "  Args:\n",
        "    data_dir: 数据集所在的目录路径。\n",
        "    subfolder_name: 子目录的名称，默认为 \"images\"。\n",
        "  \"\"\"\n",
        "\n",
        "  # 创建子目录\n",
        "  subfolder_path = os.path.join(data_dir, subfolder_name)\n",
        "  os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "  # 遍历数据目录下的所有文件\n",
        "  for filename in os.listdir(data_dir):\n",
        "    # 检查文件是否为图像文件\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "      # 获取文件的完整路径\n",
        "      source_path = os.path.join(data_dir, filename)\n",
        "      # 获取目标路径\n",
        "      destination_path = os.path.join(subfolder_path, filename)\n",
        "      # 移动文件\n",
        "      shutil.move(source_path, destination_path)\n",
        "\n",
        "# 调用函数，将 /content/dataset/train 目录下的图像移动到 images 子目录中\n",
        "move_images_to_subfolder(\"/content/volley_detect-1/train\")\n",
        "move_images_to_subfolder(\"/content/volley_detect-1/valid\")\n",
        "move_images_to_subfolder(\"/content/volley_detect-1/test\")\n",
        "\n",
        "print(\"图像文件已移动到 images 子目录中。\")\n",
        "\n",
        "\n",
        "\n",
        "# 这个傻逼函数还预设json文件名为labels.json，只能改名"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WMt07mZxHUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于该数据集只有两个类，都是volleyball，没有背景类，而modelmaker要求有一个背景类并且编号为0，故需要做额外处理"
      ],
      "metadata": {
        "id": "1pv0icliMWNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def add_background_class_to_coco_json(json_file_path):\n",
        "  \"\"\"向 COCO JSON 文件添加背景类，并更新现有类别的 ID。\n",
        "\n",
        "  Args:\n",
        "      json_file_path: COCO JSON 文件的路径。\n",
        "  \"\"\"\n",
        "  with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "  # 添加背景类\n",
        "  data['categories'].insert(0, {'id': 0, 'name': 'background'})\n",
        "\n",
        "  # 更新现有类别的 ID\n",
        "  for category in data['categories'][1:]:  # 从索引 1 开始，跳过背景类\n",
        "    category['id'] += 1\n",
        "\n",
        "  # 更新注释中的类别 ID\n",
        "  for annotation in data['annotations']:\n",
        "    annotation['category_id'] += 1  # 所有类别 ID 递增 1\n",
        "\n",
        "  with open(json_file_path, 'w') as f:\n",
        "    json.dump(data, f, indent=2)\n",
        "\n",
        "  print(f\"已向 {json_file_path} 添加背景类，并更新了现有类别的 ID。\")\n",
        "\n",
        "# 调用函数\n",
        "add_background_class_to_coco_json(\"/content/volley_detect-1/train/labels.json\")\n",
        "add_background_class_to_coco_json(\"/content/volley_detect-1/valid/labels.json\")\n",
        "add_background_class_to_coco_json(\"/content/volley_detect-1/test/labels.json\")"
      ],
      "metadata": {
        "id": "ZrFdS9C-Mtq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdyImqyI6s-"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.Dataset.from_coco_folder(\n",
        "    \"/content/volley_detect-1/train\",\n",
        "    cache_dir=\"/tmp/od_data/train\"\n",
        "    )\n",
        "validation_data = object_detector.Dataset.from_coco_folder(\"/content/volley_detect-1/valid\", cache_dir=\"/tmp/od_data/validation\")\n",
        "test_data = object_detector.Dataset.from_coco_folder(\"/content/volley_detect-1/test\", cache_dir=\"/tmp/od_data/test\")\n",
        "print(\"train_data size: \", train_data.size)\n",
        "print(\"validation_data size: \", validation_data.size)\n",
        "print(\"test_data size: \", test_data.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 若无测试集和验证集\n",
        "存在数据集只有训练集的情况，而重新训练要求必须有验证集，在创建模型时，不传入 validation_data 参数，模型将不会进行验证。有几种处理方法"
      ],
      "metadata": {
        "id": "L6NLPHebRB9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 将训练集拆分为训练集和验证集"
      ],
      "metadata": {
        "id": "KVSjCxLJRlra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # 将 Dataset 对象转换为列表，才能使用train_test_split函数\n",
        "# data_list = list(train_data)\n",
        "\n",
        "# # 拆分列表\n",
        "# train_data_list, validation_data_list = train_test_split(data_list, test_size=0.2, random_state=42)\n",
        "\n",
        "# # 将拆分后的列表转换回 Dataset 对象\n",
        "# train_data = object_detector.Dataset(train_data_list)\n",
        "# validation_data = object_detector.Dataset(validation_data_list)\n",
        "\n",
        "# # 将 train_data 拆分为训练集和验证集\n",
        "# train_data, validation_data = train_test_split(train_data, test_size=0.2, random_state=42)  # test_size 表示验证集的比例\n",
        "\n",
        "\n",
        "\n",
        "train_data, validation_data = train_data.split(0.8)  # 80% 用于训练，20% 用于验证\n",
        "\n",
        "print(\"train_data size: \", train_data.size)\n",
        "print(\"validation_data size: \", validation_data.size)\n"
      ],
      "metadata": {
        "id": "cIRgbaRVRsSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 使用 k 折交叉验证  \n",
        "k 折交叉验证是一种更 robust 的评估模型性能的方法，它将训练集分成 k 个子集，每次使用 k-1 个子集进行训练，剩余的 1 个子集作为验证集。重复 k 次，每次使用不同的子集作为验证集，最终的性能指标是 k 次验证结果的平均值。"
      ],
      "metadata": {
        "id": "9eD9_QBTRyUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# # 创建 k 折交叉验证器\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)  # n_splits 表示将数据集分成 5 个子集\n",
        "\n",
        "# # 循环进行训练和评估\n",
        "# for train_index, val_index in kf.split(train_data):\n",
        "#   train_data_fold = train_data[train_index]  # 当前 fold 的训练集\n",
        "#   validation_data_fold = train_data[val_index]  # 当前 fold 的验证集\n",
        "\n",
        "#   # 创建模型\n",
        "#   model = object_detector.ObjectDetector.create(\n",
        "#       train_data=train_data_fold,\n",
        "#       validation_data=validation_data_fold,  # 传入当前 fold 的验证集\n",
        "#       options=options\n",
        "#   )\n"
      ],
      "metadata": {
        "id": "qwq1l-heR2jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "SWhiGXiKYRoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "det3wYoWRRDs"
      },
      "source": [
        "## 重新训练模型\n",
        "\n",
        "数据准备结束后, 可以开始重新训练模型来识别根据训练数据新定义的目标或者类。以下内容介绍使用之前准备的数据来训练模型识别新类。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9AzF_CGA7mj"
      },
      "source": [
        "### 重新训练设置项\n",
        "除了训练数据集之外，还有一些运行再训练所需的设置： 模型的输出目录和模型架构。使用 HParams 为输出目录指定`export_dir`参数。使用 SupportedModel 类指定模型架构。mediapipe对象检测器解决方案支持以下模型架构：\n",
        "* `MobileNet-V2`\n",
        "* `MobileNet-MultiHW-AVG`\n",
        "\n",
        "更多高级的训练参数自定义设置，看[Hyperparameters](#hyperparameters) section below.\n",
        "\n",
        "设置必要参数，使用以下代码："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZHjWHM1JyiN"
      },
      "outputs": [],
      "source": [
        "spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG\n",
        "hparams = object_detector.HParams(export_dir='/content/exported_model')\n",
        "options = object_detector.ObjectDetectorOptions(\n",
        "    supported_model=spec,\n",
        "    hparams=hparams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Kto1RlCcPj"
      },
      "source": [
        "### 运行 重新训练\n",
        "With your training dataset and retraining options prepared, you are ready to start the retraining process. This process is resource intensive and can take a few minutes to a few hours depending on your available compute resources. Using a Google Colab environment with standard GPU runtimes, the example retraining below takes about 2~4 minutes.\n",
        "\n",
        "To begin the retraining process, use the `create()` method with dataset and options you previously defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5bIsWBZCb8d"
      },
      "outputs": [],
      "source": [
        "# 创建模型\n",
        "model = object_detector.ObjectDetector.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,  # 传入验证集\n",
        "    options=options\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuRapoFiRp34"
      },
      "source": [
        "### 评估模型表现\n",
        "\n",
        "在训练模型之后，在验证集上评估并打印出loss和coco_metrics,评估模型表现最重要的度量是 typically the \"AP\" coco metric for Average Precision."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上传结果到comet"
      ],
      "metadata": {
        "id": "kNk0uSSoZvP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_model(experiment, model=model, model_name=\"mediapipe_detect\")"
      ],
      "metadata": {
        "id": "bC3aeeCbZsfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "本地评估下"
      ],
      "metadata": {
        "id": "7pMXglRRZ0vI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJvB_nf7RwzJ"
      },
      "outputs": [],
      "source": [
        "loss, coco_metrics = model.evaluate(validation_data, batch_size=4)\n",
        "print(f\"Validation loss: {loss}\")\n",
        "print(f\"Validation coco metrics: {coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax8TkYA9VJUv"
      },
      "source": [
        "## Export model\n",
        "\n",
        "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWZqnGEKVP13"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model\n",
        "files.download('exported_model/mediapipe_detect.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYu5ENbT4T6"
      },
      "source": [
        "## 模型量化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIeJjCfWTnBj"
      },
      "source": [
        "Model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy.\n",
        "\n",
        "This section of the guide explains how to apply quantization to your model. Model Maker supports two forms of quantization for object detector:\n",
        "1. Quantization Aware Training: 8 bit integer precision for CPU usage\n",
        "2. Post-Training Quantization: 16 bit floating point precision for GPU usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcB3DRfHWDfs"
      },
      "source": [
        "### Quantization aware training (int8 quantization)\n",
        "Quantization aware training (QAT) is a fine-tuning step which happens after fully training your model. This technique further tunes a model which emulates inference time quantization in order to account for the lower precision of 8 bit integer quantization. For on-device applications with a standard CPU, use Int8 precision. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/training) documentation.\n",
        "\n",
        "To apply quantization aware training and export to an int8 model, create a `QATHParams` configuration and run the `quantization_aware_training` method. See the **Hyperparameters** section below on detailed usage of `QATHParams`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7nRSQT9WCS-"
      },
      "outputs": [],
      "source": [
        "qat_hparams = object_detector.QATHParams(learning_rate=0.3, batch_size=4, epochs=10, decay_steps=6, decay_rate=0.96)\n",
        "model.quantization_aware_training(train_data, validation_data, qat_hparams=qat_hparams)\n",
        "qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n",
        "print(f\"QAT validation loss: {qat_loss}\")\n",
        "print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT7grgHOW048"
      },
      "source": [
        "The QAT step often requires multiple runs to tune the parameters of training. To avoid having to rerun model training using the `create` method, use the `restore_float_ckpt` method to restore the model state back to the fully trained float model(After running the `create` method) in order to run QAT again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHDPWbIaXjR4"
      },
      "outputs": [],
      "source": [
        "new_qat_hparams = object_detector.QATHParams(learning_rate=0.9, batch_size=4, epochs=15, decay_steps=5, decay_rate=0.96)\n",
        "model.restore_float_ckpt()\n",
        "model.quantization_aware_training(train_data, validation_data, qat_hparams=new_qat_hparams)\n",
        "qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n",
        "print(f\"QAT validation loss: {qat_loss}\")\n",
        "print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfWo6TVpWJfr"
      },
      "source": [
        "Finally, us the `export_model` to export to an int8 quantized model. The `export_model` function will automatically export to either float32 or int8 model depending on whether `quantization_aware_training` was run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsixePxCWJDp"
      },
      "outputs": [],
      "source": [
        "model.export_model('model_int8_qat.tflite')\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_int8_qat.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO8nZR3Cgx8_"
      },
      "source": [
        "### Post-training quantization (fp16 quantization)\n",
        "\n",
        "Post-training model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy. This approach reduces the size of the data processed by the model, for example by transforming 32-bit floating point numbers to 16-bit floats. Float16 quantization is reccomended for GPU usage. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) documentation.\n",
        "\n",
        "First, import the MediaPipe Model Maker quantization module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo7cQ_N-ZE8A"
      },
      "outputs": [],
      "source": [
        "from mediapipe_model_maker import quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE8j5cloZTo-"
      },
      "source": [
        "Define a QuantizationConfig object using the `for_float16()` class method. This configuration modifies a trained model to use 16-bit floating point numbers instead of 32-bit floating point numbers. You can further customize the quantization process by setting additional parameters for the QuantizationConfig class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbwc3g_Fa3dv"
      },
      "outputs": [],
      "source": [
        "quantization_config = quantization.QuantizationConfig.for_float16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTrkDXi8bM_L"
      },
      "source": [
        "Export the model using the additional quantization_config object to apply post-training quantization. Note that if you previously ran `quantization_aware_training`, you must first convert the model back to a float model by using `restore_float_ckpt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmzEu_AjbMPI"
      },
      "outputs": [],
      "source": [
        "model.restore_float_ckpt()\n",
        "model.export_model(model_name=\"model_fp16.tflite\", quantization_config=quantization_config)\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_fp16.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npaRBUB3ZevY"
      },
      "source": [
        "## Hyperparameters\n",
        "You can further customize the model using the ObjectDetectorOptions class, which has three parameters for `SupportedModels`, `ModelOptions`, and `HParams`.\n",
        "\n",
        "Use the `SupportedModels` enum class to specify the model architecture to use for training. The following model architectures are supported:\n",
        "* MOBILENET_V2\n",
        "* MOBILENET_V2_I320\n",
        "* MOBILENET_MULTI_AVG\n",
        "* MOBILENET_MULTI_AVG_I384\n",
        "\n",
        "Use the `HParams` class to customize other parameters related to training and saving the model:\n",
        "* `learning_rate`: Learning rate to use for gradient descent training. Defaults to 0.3.\n",
        "* `batch_size`: Batch size for training. Defaults to 8.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 30.\n",
        "* `cosine_decay_epochs`: The number of epochs for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to None, which is equivalent to setting it to `epochs`.\n",
        "* `cosine_decay_alpha`: The alpha value for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to 1.0, which means no cosine decay.\n",
        "\n",
        "Use the `ModelOptions` class to customize parameters related to the model itself:\n",
        "* `l2_weight_decay`: L2 regularization penalty used in [tf.keras.regularizers.L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2). Defaults to 3e-5.\n",
        "\n",
        "Uset the `QATHParams` class to customize training parameters for Quantization Aware Training:\n",
        "* `learning_rate`: Learning rate to use for gradient descent QAT. Defaults to 0.3.\n",
        "* `batch_size`: Batch size for QAT. Defaults to 8\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 15.\n",
        "* `decay_steps`: Learning rate decay steps for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 8\n",
        "* `decay_rate`: Learning rate decay rate for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 0.96."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HCrUl8z6liX"
      },
      "source": [
        "## Benchmarking\n",
        "Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same android figurines dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n",
        "* The android figurines dataset is a small and simple dataset with 62 training examples and 10 validation examples. Since the dataset is quite small, metrics may vary drastically due to variances in the training process. This dataset was provided for demo purposes and it is recommended to collect more data samples for better performing models.\n",
        "* The float32 models were trained with the default HParams, and the QAT step for the int8 models was run with `QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_rate=1)`.\n",
        "* For your own dataset, you will likely need to tune values for both HParams and QATHParams in order to achieve the best results. See the [Hyperparameters](#hyperparameters) section above for more information on configuring training parameters.\n",
        "* All latency numbers are benchmarked on the Pixel 6.\n",
        "\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<col>\n",
        "<col>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<tr>\n",
        "<th rowspan=\"2\">Model architecture</th>\n",
        "<th rowspan=\"2\">Input Image Size</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Test AP</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">CPU Latency</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Model Size</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNetV2</td>\n",
        "<td>256x256</td>\n",
        "<td>88.4%</td>\n",
        "<td>73.5%</td>\n",
        "<td>48ms</td>\n",
        "<td>16ms</td>\n",
        "<td>11MB</td>\n",
        "<td>3.2MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNetV2 I320</td>\n",
        "<td>320x320</td>\n",
        "<td>89.1%</td>\n",
        "<td>75.5%</td>\n",
        "<td>75ms</td>\n",
        "<td>33.38ms</td>\n",
        "<td>10MB</td>\n",
        "<td>3.3MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG</td>\n",
        "<td>256x256</td>\n",
        "<td>88.5%</td>\n",
        "<td>70.0%</td>\n",
        "<td>56ms</td>\n",
        "<td>19ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG I384</td>\n",
        "<td>384x384</td>\n",
        "<td>92.7%</td>\n",
        "<td>73.4%</td>\n",
        "<td>238ms</td>\n",
        "<td>41ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOCPzKohXxy6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}